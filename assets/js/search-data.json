{
  
    
        "post0": {
            "title": "pedestrian detector",
            "content": ". https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html . pip install git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI . Collecting git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI Cloning https://github.com/gautamchitnis/cocoapi.git (to revision cocodataset-master) to /tmp/pip-req-build-0v7f304q Running command git clone -q https://github.com/gautamchitnis/cocoapi.git /tmp/pip-req-build-0v7f304q Running command git checkout -b cocodataset-master --track origin/cocodataset-master Switched to a new branch &#39;cocodataset-master&#39; Branch &#39;cocodataset-master&#39; set up to track remote branch &#39;cocodataset-master&#39; from &#39;origin&#39;. Requirement already satisfied (use --upgrade to upgrade): pycocotools==2.0 from git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI in /usr/local/lib/python3.7/dist-packages Building wheels for collected packages: pycocotools Building wheel for pycocotools (setup.py) ... done Created wheel for pycocotools: filename=pycocotools-2.0-cp37-cp37m-linux_x86_64.whl size=264249 sha256=17c35a7c36ad6a8b630d42fbc47971ad4148e14463cd2af16c4acfa13a550e51 Stored in directory: /tmp/pip-ephem-wheel-cache-vg75ykd3/wheels/49/ca/2b/1b99c52bb9e7a9804cd60d66243ec70c9f15977795793d2646 Successfully built pycocotools . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . import zipfile with zipfile.ZipFile(&quot;/content/drive/MyDrive/PennFudanPed.zip&quot;, &#39;r&#39;) as zip_ref: zip_ref.extractall() . import sys sys.path.insert(1, &#39;/content/drive/MyDrive/detection&#39;) import utils import transforms import engine . import os import numpy as np import torch from PIL import Image class PennFudanDataset(object): def __init__(self, root, transforms): self.root = root self.transforms = transforms # load all image files, sorting them to # ensure that they are aligned self.imgs = list(sorted(os.listdir(os.path.join(root, &quot;PNGImages&quot;)))) self.masks = list(sorted(os.listdir(os.path.join(root, &quot;PedMasks&quot;)))) def __getitem__(self, idx): # load images and masks img_path = os.path.join(self.root, &quot;PNGImages&quot;, self.imgs[idx]) mask_path = os.path.join(self.root, &quot;PedMasks&quot;, self.masks[idx]) img = Image.open(img_path).convert(&quot;RGB&quot;) # note that we haven&#39;t converted the mask to RGB, # because each color corresponds to a different instance # with 0 being background mask = Image.open(mask_path) # convert the PIL Image into a numpy array mask = np.array(mask) # instances are encoded as different colors obj_ids = np.unique(mask) # first id is the background, so remove it obj_ids = obj_ids[1:] # split the color-encoded mask into a set # of binary masks masks = mask == obj_ids[:, None, None] # get bounding box coordinates for each mask num_objs = len(obj_ids) boxes = [] for i in range(num_objs): pos = np.where(masks[i]) xmin = np.min(pos[1]) xmax = np.max(pos[1]) ymin = np.min(pos[0]) ymax = np.max(pos[0]) boxes.append([xmin, ymin, xmax, ymax]) # convert everything into a torch.Tensor boxes = torch.as_tensor(boxes, dtype=torch.float32) # there is only one class labels = torch.ones((num_objs,), dtype=torch.int64) masks = torch.as_tensor(masks, dtype=torch.uint8) image_id = torch.tensor([idx]) area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) # suppose all instances are not crowd iscrowd = torch.zeros((num_objs,), dtype=torch.int64) target = {} target[&quot;boxes&quot;] = boxes target[&quot;labels&quot;] = labels target[&quot;masks&quot;] = masks target[&quot;image_id&quot;] = image_id target[&quot;area&quot;] = area target[&quot;iscrowd&quot;] = iscrowd if self.transforms is not None: img, target = self.transforms(img, target) return img, target def __len__(self): return len(self.imgs) . import torchvision from torchvision.models.detection.faster_rcnn import FastRCNNPredictor # load a model pre-trained pre-trained on COCO model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) # replace the classifier with a new one, that has # num_classes which is user-defined num_classes = 2 # 1 class (person) + background # get number of input features for the classifier in_features = model.roi_heads.box_predictor.cls_score.in_features # replace the pre-trained head with a new one model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) . import torchvision from torchvision.models.detection import FasterRCNN from torchvision.models.detection.rpn import AnchorGenerator # load a pre-trained model for classification and return # only the features backbone = torchvision.models.mobilenet_v2(pretrained=True).features # FasterRCNN needs to know the number of # output channels in a backbone. For mobilenet_v2, it&#39;s 1280 # so we need to add it here backbone.out_channels = 1280 # let&#39;s make the RPN generate 5 x 3 anchors per spatial # location, with 5 different sizes and 3 different aspect # ratios. We have a Tuple[Tuple[int]] because each feature # map could potentially have different sizes and # aspect ratios anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),)) # let&#39;s define what are the feature maps that we will # use to perform the region of interest cropping, as well as # the size of the crop after rescaling. # if your backbone returns a Tensor, featmap_names is expected to # be [0]. More generally, the backbone should return an # OrderedDict[Tensor], and in featmap_names you can choose which # feature maps to use. roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[&#39;0&#39;], output_size=7, sampling_ratio=2) # put the pieces together inside a FasterRCNN model model = FasterRCNN(backbone, num_classes=2, rpn_anchor_generator=anchor_generator, box_roi_pool=roi_pooler) . import torchvision from torchvision.models.detection.faster_rcnn import FastRCNNPredictor from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor def get_model_instance_segmentation(num_classes): # load an instance segmentation model pre-trained pre-trained on COCO model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True) # get number of input features for the classifier in_features = model.roi_heads.box_predictor.cls_score.in_features # replace the pre-trained head with a new one model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) # now get the number of input features for the mask classifier in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels hidden_layer = 256 # and replace the mask predictor with a new one model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes) return model . import transforms as T def get_transform(train): transforms = [] transforms.append(T.ToTensor()) if train: transforms.append(T.RandomHorizontalFlip(0.5)) return T.Compose(transforms) . model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) dataset = PennFudanDataset(&#39;PennFudanPed&#39;, get_transform(train=True)) data_loader = torch.utils.data.DataLoader( dataset, batch_size=2, shuffle=True, num_workers=4, collate_fn=utils.collate_fn) # For Training images,targets = next(iter(data_loader)) images = list(image for image in images) targets = [{k: v for k, v in t.items()} for t in targets] output = model(images,targets) # Returns losses and detections # For inference model.eval() x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] predictions = model(x) # Returns predictions . /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) . from engine import train_one_epoch, evaluate import utils def main(): # train on the GPU or on the CPU, if a GPU is not available device = torch.device(&#39;cuda&#39;) if torch.cuda.is_available() else torch.device(&#39;cpu&#39;) # our dataset has two classes only - background and person num_classes = 2 # use our dataset and defined transformations dataset = PennFudanDataset(&#39;PennFudanPed&#39;, get_transform(train=True)) dataset_test = PennFudanDataset(&#39;PennFudanPed&#39;, get_transform(train=False)) # split the dataset in train and test set indices = torch.randperm(len(dataset)).tolist() dataset = torch.utils.data.Subset(dataset, indices[:-50]) dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:]) # define training and validation data loaders data_loader = torch.utils.data.DataLoader( dataset, batch_size=2, shuffle=True, num_workers=4, collate_fn=utils.collate_fn) data_loader_test = torch.utils.data.DataLoader( dataset_test, batch_size=1, shuffle=False, num_workers=4, collate_fn=utils.collate_fn) # get the model using our helper function model = get_model_instance_segmentation(num_classes) # move model to the right device model.to(device) # construct an optimizer params = [p for p in model.parameters() if p.requires_grad] optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005) # and a learning rate scheduler lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1) # let&#39;s train it for 10 epochs num_epochs = 10 for epoch in range(num_epochs): # train for one epoch, printing every 10 iterations train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10) # update the learning rate lr_scheduler.step() # evaluate on the test dataset evaluate(model, data_loader_test, device=device) print(&quot;That&#39;s it!&quot;) . main() . /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) Downloading: &#34;https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth&#34; to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth . Epoch: [0] [ 0/60] eta: 0:50:23 lr: 0.000090 loss: 4.3564 (4.3564) loss_classifier: 0.7056 (0.7056) loss_box_reg: 0.3926 (0.3926) loss_mask: 3.1841 (3.1841) loss_objectness: 0.0605 (0.0605) loss_rpn_box_reg: 0.0136 (0.0136) time: 50.3886 data: 0.4679 Epoch: [0] [10/60] eta: 0:35:49 lr: 0.000936 loss: 1.5900 (2.3714) loss_classifier: 0.4546 (0.4669) loss_box_reg: 0.2550 (0.2915) loss_mask: 0.7683 (1.5655) loss_objectness: 0.0405 (0.0412) loss_rpn_box_reg: 0.0028 (0.0064) time: 42.9868 data: 0.0485 Epoch: [0] [20/60] eta: 0:29:25 lr: 0.001783 loss: 1.0068 (1.6108) loss_classifier: 0.2335 (0.3285) loss_box_reg: 0.2550 (0.2949) loss_mask: 0.4494 (0.9535) loss_objectness: 0.0213 (0.0283) loss_rpn_box_reg: 0.0032 (0.0056) time: 43.8188 data: 0.0074 Epoch: [0] [30/60] eta: 0:22:08 lr: 0.002629 loss: 0.5893 (1.2825) loss_classifier: 0.1033 (0.2525) loss_box_reg: 0.2704 (0.2890) loss_mask: 0.2122 (0.7117) loss_objectness: 0.0126 (0.0229) loss_rpn_box_reg: 0.0063 (0.0064) time: 44.9881 data: 0.0088 . KeyboardInterrupt Traceback (most recent call last) &lt;ipython-input-37-263240bbee7e&gt; in &lt;module&gt;() -&gt; 1 main() &lt;ipython-input-36-66e4964d4996&gt; in main() 47 for epoch in range(num_epochs): 48 # train for one epoch, printing every 10 iterations &gt; 49 train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10) 50 # update the learning rate 51 lr_scheduler.step() /content/drive/MyDrive/detection/engine.py in train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq) 44 45 optimizer.zero_grad() &gt; 46 losses.backward() 47 optimizer.step() 48 /usr/local/lib/python3.7/dist-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph, inputs) 243 create_graph=create_graph, 244 inputs=inputs) --&gt; 245 torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs) 246 247 def register_hook(self, hook): /usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs) 145 Variable._execution_engine.run_backward( 146 tensors, grad_tensors_, retain_graph, create_graph, inputs, --&gt; 147 allow_unreachable=True, accumulate_grad=True) # allow_unreachable flag 148 149 KeyboardInterrupt: .",
            "url": "https://anders447.github.io/sample-ds-blog-anders/2021/05/09/Pedestrian_detector.html",
            "relUrl": "/2021/05/09/Pedestrian_detector.html",
            "date": " • May 9, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Car detector",
            "content": ". !pip3 install git+https://github.com/fastai/fastai.git@06848de7904f6657ac5624082694915628762a2d import fastbook fastbook.setup_book() . Collecting git+https://github.com/fastai/fastai.git@06848de7904f6657ac5624082694915628762a2d Cloning https://github.com/fastai/fastai.git (to revision 06848de7904f6657ac5624082694915628762a2d) to /tmp/pip-req-build-q0ylc9eg Running command git clone -q https://github.com/fastai/fastai.git /tmp/pip-req-build-q0ylc9eg Running command git checkout -q 06848de7904f6657ac5624082694915628762a2d Requirement already satisfied (use --upgrade to upgrade): fastai==2.3.1 from git+https://github.com/fastai/fastai.git@06848de7904f6657ac5624082694915628762a2d in /usr/local/lib/python3.7/dist-packages Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (19.3.1) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (20.9) Requirement already satisfied: fastcore&lt;1.4,&gt;=1.3.8 in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (1.3.20) Collecting torchvision&lt;0.9,&gt;=0.8.2 Downloading https://files.pythonhosted.org/packages/94/df/969e69a94cff1c8911acb0688117f95e1915becc1e01c73e7960a2c76ec8/torchvision-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (12.8MB) |████████████████████████████████| 12.8MB 10.7MB/s Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (3.2.2) Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (1.1.5) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (2.23.0) Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (3.13) Requirement already satisfied: fastprogress&gt;=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (1.0.0) Requirement already satisfied: pillow&gt;6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (7.1.2) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (0.22.2.post1) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (1.4.1) Requirement already satisfied: spacy&lt;3 in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (2.2.4) Collecting torch&lt;1.8,&gt;=1.7.0 Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB) |████████████████████████████████| 776.8MB 25kB/s Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;fastai==2.3.1) (2.4.7) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision&lt;0.9,&gt;=0.8.2-&gt;fastai==2.3.1) (1.19.5) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;fastai==2.3.1) (2.8.1) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;fastai==2.3.1) (1.3.1) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;fastai==2.3.1) (0.10.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;fastai==2.3.1) (2018.9) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;fastai==2.3.1) (2020.12.5) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;fastai==2.3.1) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;fastai==2.3.1) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;fastai==2.3.1) (1.24.3) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;fastai==2.3.1) (1.0.1) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3-&gt;fastai==2.3.1) (2.0.5) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3-&gt;fastai==2.3.1) (56.0.0) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3-&gt;fastai==2.3.1) (0.8.2) Requirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3-&gt;fastai==2.3.1) (4.41.1) Requirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3-&gt;fastai==2.3.1) (1.0.5) Requirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3-&gt;fastai==2.3.1) (1.0.0) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3-&gt;fastai==2.3.1) (3.0.5) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3-&gt;fastai==2.3.1) (1.0.5) Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3-&gt;fastai==2.3.1) (7.4.0) Requirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3-&gt;fastai==2.3.1) (0.4.1) Requirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;3-&gt;fastai==2.3.1) (1.1.3) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch&lt;1.8,&gt;=1.7.0-&gt;fastai==2.3.1) (3.7.4.3) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib-&gt;fastai==2.3.1) (1.15.0) Requirement already satisfied: importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;3-&gt;fastai==2.3.1) (3.10.1) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34;-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;3-&gt;fastai==2.3.1) (3.4.1) Building wheels for collected packages: fastai Building wheel for fastai (setup.py) ... done Created wheel for fastai: filename=fastai-2.3.1-cp37-none-any.whl size=192786 sha256=e3efda78d22c2f54c04e06dea7c3aa817f6d6063b0f5d17c0b0b29a3583e7d44 Stored in directory: /tmp/pip-ephem-wheel-cache-dlfo4tbz/wheels/43/2f/2f/e251e2c1c0f1d4c39beb16e4ea616250e8b21766a2a3b1281a Successfully built fastai ERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you&#39;ll have torch 1.7.1 which is incompatible. Installing collected packages: torch, torchvision Found existing installation: torch 1.8.1+cu101 Uninstalling torch-1.8.1+cu101: Successfully uninstalled torch-1.8.1+cu101 Found existing installation: torchvision 0.9.1+cu101 Uninstalling torchvision-0.9.1+cu101: Successfully uninstalled torchvision-0.9.1+cu101 Successfully installed torch-1.7.1 torchvision-0.8.2 . import zipfile with zipfile.ZipFile(&quot;/content/gdrive/MyDrive/SelfDrivingCarDataset.zip&quot;, &#39;r&#39;) as zip_ref: zip_ref.extractall() . path = Path(&quot;/content/export&quot;) Path.BASE_PATH = path path.ls().sorted() . (#30000) [Path(&#39;1478019952686311006_jpg.rf.a8dc9db36ae4a7e6ecf234f85885a153.jpg&#39;),Path(&#39;1478019952686311006_jpg.rf.a8dc9db36ae4a7e6ecf234f85885a153.xml&#39;),Path(&#39;1478019953180167674_jpg.rf.a867d5d390d81f5ccfb855463711bc5b.jpg&#39;),Path(&#39;1478019953180167674_jpg.rf.a867d5d390d81f5ccfb855463711bc5b.xml&#39;),Path(&#39;1478019953689774621_jpg.rf.3cc61c5c050dfd04456823d102bdfdf1.jpg&#39;),Path(&#39;1478019953689774621_jpg.rf.3cc61c5c050dfd04456823d102bdfdf1.xml&#39;),Path(&#39;1478019954186238236_jpg.rf.eebc37d396bcff8efdda2dcccb23a267.jpg&#39;),Path(&#39;1478019954186238236_jpg.rf.eebc37d396bcff8efdda2dcccb23a267.xml&#39;),Path(&#39;1478019954685370994_jpg.rf.f64f429db0c352326724fddc95fa23cb.jpg&#39;),Path(&#39;1478019954685370994_jpg.rf.f64f429db0c352326724fddc95fa23cb.xml&#39;)...] . img_files = get_image_files(path) . def img2xml(x): return Path(f&#39;{str(x)[:-3]}xml&#39;) def get_label(x): result = [] tree = ET.parse(img2xml(x)) root = tree.getroot() for CarObject in root.iter(&#39;object&#39;): if CarObject[0].text == &#39;car&#39;: result.append(&#39;&#39;) if not result: result.append(&#39;&#39;) return result def get_bbx(x): tree = ET.parse(img2xml(x)) root = tree.getroot() bbox = [] for CarObject in root.iter(&#39;object&#39;): if CarObject[0].text == &#39;car&#39;: for boxes in CarObject.iter(&#39;bndbox&#39;): bbox += [[int(boxes[1].text), int(boxes[3].text),int(boxes[0].text),int(boxes[2].text)]] if not bbox: bbox += [[0,0,0,0]] return bbox get_y = [ get_bbx, get_label ] . . cars = DataBlock( blocks=(ImageBlock, BBoxBlock, BBoxLblBlock), splitter=RandomSplitter(valid_pct=0.2, seed=42), get_items=get_image_files, n_inp=1, get_y=get_y, item_tfms=Resize(512) ) . dls = cars.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . .",
            "url": "https://anders447.github.io/sample-ds-blog-anders/2021/05/08/_car_detector.html",
            "relUrl": "/2021/05/08/_car_detector.html",
            "date": " • May 8, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "test colab",
            "content": ". 2+2 . 4 .",
            "url": "https://anders447.github.io/sample-ds-blog-anders/2021/05/08/_04_27_test.html",
            "relUrl": "/2021/05/08/_04_27_test.html",
            "date": " • May 8, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://anders447.github.io/sample-ds-blog-anders/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://anders447.github.io/sample-ds-blog-anders/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://anders447.github.io/sample-ds-blog-anders/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}